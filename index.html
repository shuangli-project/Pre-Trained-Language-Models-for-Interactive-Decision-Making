

<!DOCTYPE html>
<html lang="en">

<head>
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-111713571-1"></script>
	<script>
  		window.dataLayer = window.dataLayer || [];
  		function gtag(){dataLayer.push(arguments);}
  		gtag('js', new Date());
  		gtag('config', 'UA-111713571-1');
	</script>

	<!-- Required meta tags -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

	<!-- Bootstrap core CSS -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">
	<!-- Custom styles for this template -->
	<link href="files/jumbotron.css" rel="stylesheet">
</head>


<style>
.center2 {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}
.center3 {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 80%;
}
.center4 {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 70%;
}
.center5 {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 100%;
}
.center6 {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 35%;
}
.center7 {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 30%;
}
</style>


<title>vh-language</title>

<body>
	<nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark">
		<a class="navbar-brand" href="#">Pre-Trained Language Models for Interactive Decision-Making</a>

		<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarToggle">
	  		<span class="navbar-toggler-icon"></span>
		</button>

		<div class="collapse navbar-collapse" id="navbarToggle">
			<ul class="navbar-nav ml-auto">
				<li class="nav-item">
					<a class="nav-link" href="#">Home</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Abstract">Abstract</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Paper">Paper</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Model">Model</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Results">Results</a>
				</li>
			</ul>
		</div>
	</nav>

	<div class="container" style="padding-top: 80px; font-size: 20px">
		<div align="center">
			<h2 class="text-center" align="center">
				Pre-Trained Language Models for Interactive Decision-Making
			</h2><br>

			<a href="https://people.csail.mit.edu/lishuang/">Shuang Li<sup>1</sup></a> &nbsp;&nbsp;&nbsp;
			<a href="https://people.csail.mit.edu/xavierpuig/">Xavier Puig<sup>1</sup></a> &nbsp;&nbsp;&nbsp;
			<a href="">Chris Paxton<sup>2</sup></a> &nbsp;&nbsp;&nbsp;
			<a href="https://yilundu.github.io/">Yilun Du<sup>1</sup></a> &nbsp;&nbsp;&nbsp;
			<a href="https://clintonjwang.github.io/">Clinton Wang<sup>1</sup></a> &nbsp;&nbsp;&nbsp;
			<a href="">Linxi Fan<sup>2</sup></a> <br>
			<a href="">Tao Chen<sup>1</sup></a> &nbsp;&nbsp;&nbsp;
			<a href="">De-An Huang<sup>2</sup></a> &nbsp;&nbsp;&nbsp;
			<a href="https://www.ekinakyurek.me/">Ekin Akyürek<sup>1</sup></a> &nbsp;&nbsp;&nbsp;
			<a href="">Anima Anandkumar&dagger;<sup>2</sup></a> &nbsp;&nbsp;&nbsp; <br>
			<a href="https://www.mit.edu/~jda/">Jacob Andreas&dagger;<sup>1</sup></a> &nbsp;&nbsp;&nbsp;
			<a href="https://scholar.google.com/citations?user=Vzr1RukAAAAJ&hl=en">Igor Mordatch&dagger;<sup>3</sup></a> &nbsp;&nbsp;&nbsp;
			<a href="http://web.mit.edu/torralba/www/">Antonio Torralba&dagger;<sup>1</sup></a> &nbsp;&nbsp;&nbsp;
			<a href="">Yuke Zhu&dagger;<sup>2</sup></a> &nbsp;&nbsp;&nbsp; <br>
			<br>
			<b>Junior authors are ordered based on their contributions and senior authors&dagger; are ordered alphabetically</b>
			<br>
			<br>
			<!-- <br> -->
			<a><sup>1</sup>MIT CSAIL</a> &nbsp;&nbsp;&nbsp;&nbsp;
			<a><sup>2</sup>Nvidia</a> &nbsp;&nbsp;&nbsp;&nbsp;
			<a><sup>3</sup>Google Brain</a><br>
			<br>
			<a href="https://arxiv.org/abs/2202.01771"><b>NeurIPS 2022</b></a> &nbsp;&nbsp;&nbsp;&nbsp;
			<a href="https://github.com/ShuangLI59/Language-Model-Pre-training-Improves-Generalization-in-Policy-Learning"><b>Github</b></a><br>

			<!-- <a href="https://www.csail.mit.edu/"><b><sup>1</sup>MIT CSAIL</b></a><br> -->
			<!-- <small>(* indicate equal contribution)</small> -->

		</div>
	</div><br>


	<br>
	<!-- <br> -->
	<!-- <br><br> -->
	

	

	<div class="container">
		<hr class="my-4">
		<h3 id="tem1" style="padding-top: 80px; margin-top: -80px;">Policy generated by a pre-trained language model for a given household task</h3>
		<br>
		<video width="800" controls playsinline autoplay muted loop class=center5>
			<source src="imgs_new/teaser2.mp4" type="video/mp4">
		</video>
		<br>
		The policy learned by fine-tuning the pre-trained language model successfully finishes the task described in the goal predicates. 
		We highlight the key actions in the map, where the agent is finding, grabbing, or placing objects in the target positions.
		<br><br>
	</div>


	<br><br>


	<!-- Abstract -->
	<div class="container">
		<hr class="my-4">
		<h3 id="Abstract" style="padding-top: 80px; margin-top: -80px;">Abstract</h3>
		Language model (LM) pre-training is useful in many language processing tasks. But can pre-trained LMs be further leveraged for more general machine learning problems? We propose an approach for using LMs to scaffold learning and generalization in general sequential decision-making problems. In this approach, goals and observations are represented as a sequence of embeddings, and a policy network initialized with a pre-trained LM predicts the next action. We demonstrate that this framework enables effective combinatorial generalization across different environments and supervisory modalities. We begin by assuming access to a set of expert demonstrations, and show that initializing policies with LMs and fine-tuning them via behavior cloning improves task completion rates by 43.6% in the VirtualHome environment. We then examine how our framework may be used in environments without pre-collected expert data. To do this, we integrate an active data gathering procedure into pre-trained LMs. The agent iteratively learns by interacting with the environment, relabeling the language goal of past ``failed'' experiences, and updating the policy in a self-supervised loop. The active data gathering procedure also enables effective combinatorial generalization, outperforming the best baseline by 25.1%. Finally, we explain these results by investigating three possible factors underlying the effectiveness of the LM-based policy. We find that sequential input representations (vs. fixed-dimensional feature vectors) and favorable weight initialization are both important for generalization. Surprisingly, however, the format of the policy inputs encoding (e.g.\ as a natural language string vs. an arbitrary sequential encoding) has little influence. Together, these results suggest that language modeling induces representations that are useful for modeling not just language, but also goals and plans; these representations can aid learning and generalization even outside of language processing.
	</div>
	<br><br>
	


<!-- 
	<div class="container">
		<hr class="my-4">
		<h3 id="Paper" style="padding-top: 80px; margin-top: -80px;">Paper</h3>

		<div class="row">
			<div class="col-md-12">
			<a href="https://people.csail.mit.edu/lishuang/">Shuang Li</a>&nbsp;&nbsp;
			<a href="https://people.csail.mit.edu/xavierpuig/">Xavier Puig</a>&nbsp;&nbsp;
			<a href="https://cpaxton.github.io">Chris Paxton</a>&nbsp;&nbsp;
			<a href="https://yilundu.github.io/">Yilun Du</a>&nbsp;&nbsp;
			<a href="https://clintonjwang.github.io/">Clinton Wang</a>&nbsp;&nbsp;
			<a href="https://scholar.google.com/citations?user=sljtWIUAAAAJ&hl=en">Linxi Fan</a>&nbsp;&nbsp;
			<a href="https://taochenshh.github.io">Tao Chen</a>&nbsp;&nbsp;
			<a href="https://ai.stanford.edu/~dahuang/">De-An Huang</a>&nbsp;&nbsp;
			<a href="https://www.ekinakyurek.me/">Ekin Akyürek</a>&nbsp;&nbsp; <br>
			<a href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar&dagger;</a>&nbsp;&nbsp;
			<a href="https://www.mit.edu/~jda/">Jacob Andreas&dagger;</a>&nbsp;&nbsp;
			<a href="https://scholar.google.com/citations?user=Vzr1RukAAAAJ&hl=en">Igor Mordatch&dagger;</a> &nbsp;&nbsp;
			<a href="http://web.mit.edu/torralba/www/">Antonio Torralba&dagger;</a> &nbsp;&nbsp;
			<a href="https://www.cs.utexas.edu/~yukez/">Yuke Zhu&dagger;</a><br>

			<div style="font-size: 16px">*Junior authors are ordered based on their contributions and senior authors&dagger; are ordered alphabetically</div><br>

			<b style="font-size: 20px">Pre-Trained Language Models for Interactive Decision-Making</b><br>

			arxiv 2022 
			<a href="https://arxiv.org/abs/2202.01771">[Paper]</a>
			<a href="https://github.com/ShuangLI59/Pre-Trained-Language-Models-for-Interactive-Decision-Making">[Code]</a>

			</div>
		</div>
	</div>
	<br>
	<br>

	 -->

	<div class="container">
		<hr class="my-4">
		<h3 id="results" style="padding-top: 80px; margin-top: -80px;">More results</h3>
		<br>

		<b>Qualitative results of our model on VirtualHome and BabyAI.</b> We only show a sub-trajectory in each example to save space. The interacted objects are labelled by green bounding boxes.
		<img class="img-responsive img-rounded" src="imgs_new/qualitative3.png" style="width:100%; height:100%" alt="">

		<br><br>
		
		<b>Failure cases.</b> We show failure cases caused by the grounding error and policy error. We only show a sub-trajectory in each example and omit most exploration actions to save space. The interacted objects are labelled by green bounding boxes.
		<img class="img-responsive img-rounded" src="imgs_new/qualitative4.png" style="width:100%; height:100%" alt="">

	</div>

	<br>


	<div class="container">
		<hr class="my-4">
		<h3 id="Model" style="padding-top: 80px; margin-top: -80px;">Can pre-trained language models be used as a general framework for tasks across different environments?</h3>
		<br>

		In this paper, we study this question through the lens of <b>embodied decision-making</b>, investigating the effectiveness of LM pre-training as a general framework for learning policies across a variety of environments.

		<br><br>
		We propose to use <b>pre-trained language models as a general framework</b> for interactive decision-making across a variety of environments by converting all policy inputs into sequential data.
		
		<br><br>
		This framework is generic, accommodating goals and environment states represented as natural language strings, image patches, or scene graphs.

		<br><br>

		<img class="img-responsive img-rounded" src="imgs_new/1_framework3.png" style="width:100%; height:100%" alt="">

		<br><br>




		<hr class="my-4">
		<h3 id="" style="padding-top: 80px; margin-top: -80px;">Learning without pre-collected expert data</h3>

		We further examine how our method may be used in environments, where the expert data is not available and an agent must actively gather data from the surrounding environment. To do this, we integrate an <b>Active Data Gathering (ADG)</b> procedure into pretrained LMs.

		<br><br>

		ADG consists of three parts. First, <b>exploration</b> collects trajectories using a mix of random actions and actions generated by the current policy. Exploration is insufficient in this high dimensional problem and most of the trajectories will likely fail to achieve the end goal. A key insight is that even the failed trajectories contain useful sub-trajectories that solve certain sub-goals, and we relabel their goal in the <b>hindsight relabeling</b> stage. The relabeled goal describes what was achieved in the extracted sub-trajectory. <b>Policy update</b> samples relabeled trajectories to update the policy.

		<br><br>

		<img class="img-responsive img-rounded" src="imgs_new/im_algo8.png" style="width:100%; height:100%" alt="">

		<br><br>






		<hr class="my-4">
		<h3 id="Results" style="padding-top: 80px; margin-top: -80px;">Combinatorial generalization to out-of-distribution tasks</h3>

		We find that using pre-trained LMs as policy initializers improves in-domain performance and enables several forms of strong generalization over tasks.
		For i.i.d. training and evaluation tasks, we find that this approach yields 20% more successful policies than other baseline methods in VirtualHome.

		<br><br>

		For combinatorial generalization to out-of-distribution tasks, i.e. tasks involving new combinations of goals, states or objects, we find that LM pre-training confers even more benefits: it improves task completion rates by 43.6% for tasks involving novel goals.

		<br><br>

		<div class="center2">
			<img class="img-responsive img-rounded" src="imgs_new/1baselines.png" style="width:80%; height:100%" alt="">
		</div>







		<br>



		<hr class="my-4">
		<h3 id="Results" style="padding-top: 80px; margin-top: -80px;">Pre-trained Language Model with Active Data Gathering (LID-ADG)</h3>

		We compare LID-ADG, the proposed LM framework for decision-making using actively gathered data, to a variety of baselines that do not use pre-collected expert data on VirtualHome.
		LID-ADG (Ours) outperforms all the baselines.
		<br><br>

		<div class="center6">
			<img class="img-responsive img-rounded" src="imgs_new/adg.png" style="width:100%; height:100%" alt="">
		</div>

		<br><br>
		




		<hr class="my-4">
		<h3 style="padding-top: 80px; margin-top: -80px;">Is the effective combinatorial generalization because LMs are effective models of relations between natural language descriptions of states and actions, or because they provide a more general framework for combinatorial generalization in decision-making?</h3>

		We hypothesize and investigate three possible factors underlying the effectiveness of language modeling for generalization in policy learning:
		(1) input encoding scheme;
		(2) sequential input representations;
		and (3) favorable weight initialization.

		<br><br>

		
	
		<h4 style="padding-top: 80px; margin-top: -80px;">(1) Input encoding scheme</h4>
		We investigate (1) by encoding the environment as different types of sequences. Different input encoding schemes have only a negligible impact on model performance: the effectiveness of language modeling is not limited to utilizing natural strings, but in fact extends to arbitrary sequential encodings.

		<br><br>
		<div class="center6">
			<img class="img-responsive img-rounded" src="imgs_new/table1.png" style="width:100%" alt="">
		</div>

		<br>
		Success rates of policies trained with different input encodings in the Novel Tasks setting on VirtualHome. The text encoding is most sample-efficient, but all models converge to similar performance given sufficient training data.
		

		
		<br><br>

		<h4 style="padding-top: 80px; margin-top: -80px;">(2) Sequential input representations</h4>
		We investigate (2) by encoding observations with a single vector embedding, thereby removing its sequential structure (No-Seq). This operation significantly hurts the model's performance on novel tasks.

		
		<br><br>

		<h4 style="padding-top: 80px; margin-top: -80px;">(3) Parameter pre-training</h4>
		Finally, we investigate (3) by learning the parameters of the policy network from scratch (No-Pretrain). The success rate on novel tasks after removing the pre-trained LM weights drops by 11.2%.

		<br><br>
		<div class="center7">
			<img class="img-responsive img-rounded" src="imgs_new/table3.png" style="width:100%" alt="">
		</div>
		<br>

		"LID-Text (Ours)"" refines a pre-trained LM while "No-Pretrain" learns it from scratch. "No-FT" freezes the pre-trained weights. "No-Seq" uses non-sequential inputs. Fine-tuning the pre-trained weights and the usage of sequential encoding are important for combinatorial generalization.



		<br><br>
		
		We find that sequential input representations (vs. fixed-dimensional feature vectors) and favorable weight initialization are both important for generalization, however, the input encoding schemes (e.g. as a natural language string vs. an arbitrary encoding scheme) has little influence.


	</div>





	<!-- <div class="container">
		<hr class="my-4">
		<h3 id="tem1" style="padding-top: 80px; margin-top: -80px;">Policy generated by a pre-trained language model for a given household task</h3>
		The policy learned by fine-tuning the pre-trained language model successfully finishes the task described in the goal predicates. 
		We highlight the key actions in the map, where the agent is finding, grabbing, or placing objects in the target positions.
		<br><br><br>
		<img class="img-responsive img-rounded" src="imgs_new/teaser1.gif" style="width:100%; height:100%" alt="">
	</div>


	<br><br> -->


	<!-- Video -->
	<div class="container">
		<hr class="my-4">
		<h3 style="padding-top: 80px; margin-top: -80px;">More Qualitative Results</h3><br>
		<!-- <br> -->


		
		<div align="left">
			<h4 style="padding-top: 80px; margin-top: -80px;">1. Policy with pre-trained Language Model v.s. Policy without pre-trained Language Model</h4>
		  <!-- <div class="embed-responsive embed-responsive-16by9"> -->
		    <!-- <iframe style="border: 1px solid #000;" class="embed-responsive-item" src="imgs_new/vid1.mp4" frameborder="0" allowfullscreen></iframe> -->
		  <!-- </div> -->
		  	<br>
		  	<video width="800" controls playsinline autoplay muted loop class=center4>
			  <source src="imgs_new/vid1.mp4" type="video/mp4">
			</video>
		</div>
		<br>
		<br>


		<div align="left">
			<h4 style="padding-top: 80px; margin-top: -80px;">2. Policy with pre-trained Language Model v.s. Policy with LSTM</h4>
			<br>
		  	<video width="800" controls playsinline autoplay muted loop class=center4>
			  <source src="imgs_new/vid2.mp4" type="video/mp4">
			</video>
		</div>
		<br>
		<br>

		<div align="left">
			<h4 style="padding-top: 80px; margin-top: -80px;">3. Policy with pre-trained Language Model on different test settings</h4>
			<br>
		  	<video width="800" controls playsinline autoplay muted loop class=center4>
			  <source src="imgs_new/vid3.mp4" type="video/mp4">
			</video>
		</div>
		<br>


	</div>
	<br><br><br><br>



<!-- 
  	<div class="container">
		<h3 id="Model" style="padding-top: 80px; margin-top: -80px;">Model Overview</h3>
		<br>

		<h4 style="padding-top: 80px; margin-top: -80px;">1. Overview of VirtualHome</h4>
		<br>
	  	<img class="img-responsive img-rounded" src="imgs/environment.png" style="width:100%; height:100%" alt="">
		<br><br><br>

		<h4 style="padding-top: 80px; margin-top: -80px;">2. Overview of the training procedure</h4>
		<br>
	  	<img class="img-responsive img-rounded" src="imgs/model3.png" style="width:100%; height:100%" alt="">
		<br><br><br>
	</div>
	<br><br><br><br>
 -->


	<div class="container">
		<hr>
		<center>
			<footer>
				<p>&copy; Massachusetts Institute of Technology 2022</p>
			</footer>
		</center>
	</div>


	<!-- Bootstrap core JavaScript -->
	<!-- Placed at the end of the document so the pages load faster -->
	<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" crossorigin="anonymous"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" crossorigin="anonymous"></script>
</body>

<

